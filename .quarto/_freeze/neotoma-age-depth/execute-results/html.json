{
  "hash": "3561206394f47a57a2788eac889c2bf3",
  "result": {
    "engine": "knitr",
    "markdown": "# Background {#sec-age-depth}\n\nFor the last 50,000 years, radiocarbon (^14^C), with its half-life of 5,730 years, is by far the most common form of radiometric dating. (Beyond 10 half-lives, so much of a radioactive substance has decayed away that it becomes immeasurable.) Radiocarbon is the mainstay of Quaternary dating and archaeology.\n\nIn Quaternary paleoecology, radiocarbon dating is expensive – a single sample typically costs $300 to $500 – so usually a given lake-sediment record will have only a scattering (ca. 5 to 30) of radiocarbon dates and other age controls. Other kinds of age controls include volcanic ash layers (tephras), ^210^Pb (half-life: 22.6 yrs), optically stimulated luminescence (OSL) dates, historic events such as the rise in *Ambrosia* pollen abundances associated with EuroAmerican land clearance, etc., an age-depth model must be constructed to estimate the age of sediments not directly associated with an age control. In multi-site data syntheses, the number of age controls, their precision, and their likely accuracy are all fundamental indicators of data quality [e.g. @blois2011; @mottl2021].\n\nTo estimate ages for depths lacking radiocarbon date, an age-depth model is required. Age-depth models are fitted to the available age controls (age estimates with uncertainty for individual depths) and provide estimates of age as a function of depth, for all depths and ages within the temporal bounds of the model.\n\nHere we will gain practice in working with age-depth models of various kind, and assessing their age estimates and uncertainty. We’ll begin with a bit of practice in calibrating radiocarbon years to calendar years and comparing the resulting age estimates from different calibration curves. \n\n\n::: {.callout-note}\n# Packages required\n\nWe will be using `Bchron` for calibration and Bayesian age-depth modelling. Notably `rbacon` is another commonly used package, see @trachsel2017 for a discussion on age-depth models. We will also be fitting some interpolation and linear models using BASE-R. Remember you must have packages both installed _and_ loaded, often loading a package is done via the `library(packagename)` function. The following code using the `pacman` package takes care of installation and loading in one go.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load up the package\nif (!require(\"pacman\")) install.packages(\"pacman\", repos=\"http://cran.r-project.org\")\npacman::p_load(neotoma2, Bchron, splines, ggplot2, dplyr) # splines comes with BASE-R\n```\n:::\n\n:::\n\n\n## Radiocarbon callibration and uncertainty {#sec-cal-curve}\n\nA complication in radiocarbon dating is that the initial calculation of a radiocarbon age assumes, by convention, that the amount of radiocarbon in the atmosphere is constant over time. See @ramsey2008 for a good overview of ^14^C dating. This assumption is untrue, so all radiocarbon age estimates must be post-hoc calibrated using a calibration curve that is based on compiling radiocarbon dates of materials that have precise independent age estimates (e.g. tree rings, corals). Yet another complication in radiocarbon dating is that different calibration curves need to be used for the Northern vs. Southern Hemisphere and for the atmosphere vs. oceans, due to different residence times of ^14^C in these different reservoirs. For example, atmospheric radiocarbon that diffuses into the surface oceans usually will reside for centuries before phytoplankton biologically fix it through photosynthesis, which will lead marine ^14^C to be depleted (and ‘too old’) relative to atmospheric ^14^C. Use the wrong calibration curve and your age estimate will be inaccurate!\n\nThe IntCal series (IntCal04, IntCal09, IntCal13, IntCal20) is the community standard for calibrating radiocarbon dates to age estimates in calendar years in the northern hemisphere [@reimer2020]. The SHcal series is used for the southern hemisphere [@hogg2020]. The conversion from radiocarbon to calendar years usually further increases the uncertainty of age estimates. Here is an example of a single date:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBchronCalibrate(ages = 11553, \n                ageSds = 50, \n                calCurves = 'intcal20') |>\nplot(includeCal = TRUE)\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n# Calibrating radiocarbon dates in R {#sec-cal-dates}\n\nHere we’ll experiment with calibrating radiocarbon dates, using various calibration curves. Radiocarbon dated samples come back from the lab with a radiocarbon age and standard deviation, among other information. These two bits of information are used to calibrate the radiocarbon dates to estimated ages. R packages may have useful vignettes (package tutorials) and built-in datasets that provide handy test templates. The following code is modified from the `Bchron` [vignette](https://cran.r-project.org/web/packages/Bchron/vignettes/Bchron.html).\n\n::: {.panel-tabset}\n\n## Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nages <- BchronCalibrate(ages = c(3445, 11553, 7456), \n                        ageSds = c(50, 230, 110), \n                        calCurves = c('intcal20','intcal20','intcal20'))\n```\n:::\n\n\n## Summary\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% Highest density regions for Date1\n$`94.4%`\n[1] 3572 3834\n\n\n95% Highest density regions for Date2\n$`0.4%`\n[1] 13004 13025\n\n$`77.9%`\n[1] 13059 13874\n\n$`16.4%`\n[1] 13923 14012\n\n\n95% Highest density regions for Date3\n$`94.6%`\n[1] 8022 8423\n```\n\n\n:::\n:::\n\n\n## Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ages)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/BchronCalibratePlot-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n[[2]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/BchronCalibratePlot-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n[[3]]\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/BchronCalibratePlot-3.png){width=672}\n:::\n:::\n\n\n## Calibration curve plots \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(ages, includeCal = TRUE, fillCol = 'red')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`height` was translated to `width`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/BchronCalibrateCalPlot-1.png){width=672}\n:::\n:::\n\n\n:::\n\nThe output summary indicates the range of the highest density regions (i.e., the most likely 'real' age range of the sample). The `plot` function in `Bchron` outputs a `ggplot` object of the high density regions of the most likely ages. \n\n\n\n::: {.callout-note}\n# Note :fire:\n\n`BchronCalibrate()` is useful for checking out calibrating dates, but it is not necessary to do this step separately before running the age-depth model because the `Bchronology()` function includes calibration.\n\n:::\n\n## Your turn\n\nThe `BchronCalibrate()` example above uses three inputs: (i) uncalibrated ages (the estimated date returned from the lab); (ii) a standard deviation associated with each of the ages; (iii) a calibration curve associated with each of the dates. Take 5 minutes to:\n\n-  Experiment with calibrating a few different ages and standard deviations\n    - try a few single ages and plot the calibration curve as shown in @sec-cal-curve\n    - also try a few dates at once as in @sec-cal-dates\n- check the function documentation for different calibration curves and try the southern hemisphere calibration.\n\nGive a :raised_hand: in zoom if you have questions and :white_check_mark: when you have managed to calibrate and inspect a few different dates.\n\nWe will go through a couple of examples together.\n\n# Types of Age-Depth Models\n\nDifferent kinds of age-depth models exist, each with their own underlying assumptions and behavior. In the list below, #1-4 are classical or traditional forms of age-depth models, but Bayesian models are now the norm. The packages `rbacon` (usually referred to as 'bacon') and `Bchron` are the current standards for Bayesian age-depth modelling. Before going to bayesian models, we’ll begin with the classics.\n\n1. **Linear interpolation**, a.k.a. ‘connect the dots,’ in which straight lines are drawn between each depth-adjacent pair of age controls.\n\n2. **Linear regression** ($y=b0~ + b1x$; $y=$time and $x=$depth; $b0$ and $b1$ are constants), in which a single straight line is fitted through the entire set of age controls. In ordinary linear regression (OLS), the chosen line will minimize the y-dimension distances of individual points to the line. Standard OLS assumes that all observations are normally distributed, which is a poor assumption for calibrated radiocarbon dates.\n\n3. **Polynomials**, also fitted to the entire set of age controls ($y= b0 + b1x + b2x^2 + b3x^3 + …bnx^n$), are an extension of linear regression, with additional terms for $x^2$, $x^3$, etc. Some arbitrary maximum n is chosen, usually in the range of 3 to 5. These are called 'third-order polynomials,' 'fifth-order polynomials,' etc.\n\n4. **Splines**, which are a special kind of polynomial function that are locally fitted to subsets of the age controls, and then smoothly interpolated between points. (Several different formulas can be used to generate splines; common forms include cubic, smooth, monotonic, and LOWESS).\n\n5. **Bayesian age models** (e.g. `bacon`, `bchron`, `oxcal`, etc.). Bayesian models differ in detail, but all combine a statistical representation of prior knowledge with the new data (i.e. the age controls at a site) to build an age-depth model with posterior estimates of age for any given depth. Bayesian models are now widely used because:\n    i. they allow the incorporation of prior knowledge (e.g., from coring many lakes, we now have decent estimates of typical sediment accumulation rates, @goring2012);\n    ii. they can handle highly irregular probability distribution functions such as those for radiocarbon dates after calibration; and as a result\n    iii.  they generally do a better job of describing uncertainty than traditional age-depth models.\n\n\n## Classical age-depth models\n\nClassical models are now out-dated methods [@blaauw2018], but it is useful to understand how they work as literature before the relatively recent development of Bayesian methods has relied on them. Let's explore some classical methods of age-depth modelling using one of the datasets included with the `Bchron` package. The data are from a core in Northern Ireland; Sluggan Bog @smith1991, and can be called via:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(Sluggan) # Call the data from Bchron\nprint(Sluggan) # Check out the data\n```\n:::\n\n\n::: {.panel-tabset}\n\n## Linear interpolation\n\nLinear interpolation predicts ages by simply, drawing a line between successive dated samples. This method assumes that there is a constant age-depth relationship between samples. An assumption that is unlikely to be true, especially of cores with fewer dated samples than Sluggan Moss.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninterp_ages <- approx(x = Sluggan$ages, y = Sluggan$position) # use the function approx() to interpolate between ages\nplot(x = interp_ages$x, y = interp_ages$y, type = 'l') # Plot the interpolated data\npoints(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # overlay the original age points\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/LinearInterp-1.png){width=672}\n:::\n:::\n\n\n## Linear regression\n\nLinear regression provides a line of best fit through the dated samples. This method assumes a constant age-depth relationship across all samples, also unlikely to be true depending on processes affecting the core during its formation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_ages <- lm(Sluggan$position ~ Sluggan$ages) # Create a linear regression model\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red') # Plot the original ages\nabline(mod_ages) # add the regression line from the regression model\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/LinearReg-1.png){width=672}\n:::\n:::\n\n\n## Polynomial regression\n\nPolynomial regression allows a curve to be fit through the data. The amount the curve 'wiggles' depends on the order of the polynomial fit to the data. Polynomial regression has the risk of being over-fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- Sluggan$ages # Renaming the variables because the predict function below is fussy about the input name\ny <- Sluggan$position\npoly_ages <- lm(y ~ poly(x, 3))\n\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nage_range <- seq(from = range(Sluggan$ages)[1], to = range(Sluggan$ages)[2], length.out = 250)\nlines(age_range, predict(poly_ages, data.frame(x = age_range)))\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/PolyReg-1.png){width=672}\n:::\n:::\n\n\n## Cubic splines\n\nSplines are a class of functions including, for example, smoothing splines or cubic splines.  Cubic splines are pieve-wise polynomials locally between 'knots'. That is the data are split into bins that are fit independently using. By default, the `bs()` function uses a third degree polynomial. Without providing knots the fit will look the same as a third degree polynomial regression.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncubic_ages <- lm(y ~ bs(x, knots = c(1000, 6000, 12000)))\nplot(x = Sluggan$ages, y = Sluggan$position, cex = 1.5, col = 'red')\nlines(age_range, predict(cubic_ages, data.frame(x = age_range)))\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/CubicSpline-1.png){width=672}\n:::\n:::\n\n\n:::\n\nBecause classical age-depth modelling is rarely used now we are not going to delve further into the statistical details of the best way of fitting each model (e.g., the number of knots to use for fitting a cubic spline). \n\nOne of the issues with classical age-depth modelling is that uncertainty _decreases_ with fewer datapoints.\n\n\n# Bayesian age-depth models\n\nNow let's see what the latest methods show for the same dataset. The Sluggan dataset has a lot of radiocarbon dates! We are going to experiment by reducing the number and placement of radiocarbon samples through the core and seeing the affect on the model prediction and uncertainty.\n\n:::  {.panel-tabset}\n\n## Code\n\nNote that all the values provided to the arguments are contained in the Sluggan dataframe. When creating chronologies from your own (or accessed data), you may need to rename them to match your data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1984) # The models are stochastic, good idea to set the seed for reproducibility\nSlugganOut = with(Sluggan, \n               Bchronology(ages=ages,\n                           ageSds=ageSds, \n                           calCurves=calCurves,\n                           positions=position, \n                           positionThicknesses=thickness,\n                           ids=id, \n                           predictPositions=c(\n                             seq(min(Sluggan$position), max(Sluggan$position), by=10),\n                             518)\n                           )\n                 )\n```\n:::\n\n\n## Summary\n\nThe summary shows for each position (depth) the median and quartiles of the predicted ages for that position.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(SlugganOut)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQuantiles of predicted ages by depth: \n Depth      2.5%      25%     50%      75%     97.5%\n  44.5   630.800   824.00   883.5   942.25  1039.025\n  54.5  1057.975  1169.00  1232.5  1300.00  1454.025\n  64.5  1198.825  1337.00  1404.0  1473.00  1587.050\n  74.5  1458.975  1574.75  1642.0  1722.50  1918.025\n  84.5  1559.875  1718.75  1816.0  1898.00  2104.000\n  94.5  1708.950  1901.75  1978.0  2050.00  2220.000\n 104.5  2064.975  2169.00  2280.5  2381.00  2672.025\n 114.5  2227.925  2512.00  2648.5  2789.00  3028.050\n 124.5  2779.875  2952.00  3028.0  3117.00  3303.025\n 134.5  3052.000  3253.00  3394.0  3565.00  3916.250\n 144.5  3212.950  3496.00  3689.5  3870.50  4155.025\n 154.5  3450.750  3803.00  3976.0  4114.25  4314.000\n 164.5  3996.950  4200.00  4287.0  4372.00  4499.000\n 174.5  4265.975  4407.00  4485.5  4556.00  4690.075\n 184.5  4440.950  4571.00  4643.0  4718.00  4848.050\n 194.5  4553.975  4702.00  4777.0  4855.00  4994.025\n 204.5  4634.950  4794.00  4865.5  4942.00  5080.050\n 214.5  4716.950  4878.00  4953.0  5025.25  5156.100\n 224.5  4834.975  4978.00  5047.0  5109.00  5240.000\n 234.5  5210.000  5350.00  5422.0  5481.00  5588.225\n 244.5  5560.975  5678.75  5729.5  5787.00  5910.075\n 254.5  5665.975  5769.00  5829.5  5891.00  6003.025\n 264.5  5752.925  5878.00  5928.0  5980.00  6077.125\n 274.5  6089.925  6219.00  6279.5  6365.00  6756.125\n 284.5  6368.975  6656.50  6853.5  7069.50  7440.050\n 294.5  6701.625  7233.75  7392.0  7497.25  7661.075\n 304.5  7573.925  7714.00  7808.0  7942.25  8227.000\n 314.5  7703.975  7936.75  8078.5  8220.25  8459.125\n 324.5  7919.800  8205.00  8326.5  8447.00  8647.000\n 334.5  8420.975  8534.00  8615.0  8705.00  8887.025\n 344.5  8504.950  8647.50  8733.0  8827.00  8992.025\n 354.5  8594.000  8739.00  8835.0  8929.25  9083.175\n 364.5  8709.000  8855.00  8955.0  9035.00  9163.050\n 374.5  9125.000  9266.75  9342.0  9417.00  9575.025\n 384.5  9220.000  9352.00  9424.0  9516.00  9702.050\n 394.5  9290.000  9424.50  9508.0  9614.00  9809.125\n 404.5  9380.900  9509.00  9614.0  9718.25  9939.200\n 414.5  9555.925  9756.00  9870.0  9990.00 10195.025\n 424.5  9700.825  9955.00 10120.0 10217.00 10329.025\n 434.5  9904.875 10148.00 10292.0 10351.25 10496.225\n 444.5  9982.950 10247.75 10350.0 10419.00 10578.025\n 454.5 10547.775 10745.75 10833.0 10942.25 11120.125\n 464.5 10950.875 11221.75 11350.5 11531.00 12001.175\n 474.5 11311.650 11757.00 12011.0 12212.00 12542.100\n 484.5 12348.375 12672.00 12762.5 12831.25 12963.125\n 494.5 12801.000 12916.75 12975.0 13034.25 13160.350\n 504.5 13069.925 13226.00 13305.0 13394.25 13575.050\n 514.5 13580.875 13767.50 13873.0 13961.00 14153.200\n 518.0 13796.950 13996.75 14110.0 14230.00 14563.125\n```\n\n\n:::\n:::\n\n\n\n## Outliers\n\nThe summary shows possible outliers per date.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(SlugganOut, type = \"outliers\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPosterior outlier probability by date: \n    Date OutlierProb\n UB-201A       0.014\n UB-211A       0.013\n  UB-437       0.004\n  UB-748       0.005\n  UB-438       0.003\n  UB-439       0.006\n  UB-440       0.009\n UB-219D       0.012\n UB-219B       0.008\n UB-219A       0.010\n  UB-441       0.010\n UB-220D       0.031\n UB-220A       0.019\n UB-220E       0.018\n UB-221A       0.010\n  UB-749       0.012\n UB-223B       0.133\n UB-223A       0.020\n UB-223D       0.150\n  UB-442       0.009\n  UB-443       0.042\n UB-225B       0.203\n UB-225D       0.011\n UB-225F       0.025\n  UB-444       0.008\n UB-227D       0.008\n UB-227F       0.015\n  UB-446       0.009\n  UB-447       0.004\n UB-229D       0.012\n UB-229F       0.047\n```\n\n\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(SlugganOut)\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/BayesianPlots-1.png){width=672}\n:::\n:::\n\n\n:::\n\n::: {.callout-note}\n# Median and mean ages\n\nCommonly, only the median age is reported; however, it is good practice to report and consider the range of possible ages, especially when exploring synchroneity of events across space. Notice how `Bchron` focuses on delivering ranges and densities.\n\n:::\n\n\n## Your turn\n\nThe `Sluggan` data provide a great in-built working dataset for us to play with. Take 5 minutes to:\n\n- load up the `Sluggan` dataset (`data(Sluggan)`)\n- sample the dataset to reduce the number of rows (e.g., create a new dataset `Sluggan2 <- Sluggan[c(1, 10, 31), ]`)\n- Run and plot the `Bchron` age-depth model\n\nGive a :raised_hand: in zoom if you have questions and :white_check_mark: when you have managed to calibrate and inspect a few different dates.\n\nWho has a figure they are willing to share?\n\nLet's discuss how the output changes depending on the placement and number of chronological controls.\n\n\n- Now edit the `predictPositions` argument. This argument expects a depth sequence. Currently it is set as a sequence for every 10 cm (plus the core bottom at 518 cm). Try different depth resolutions (careful of the run time, if you go for a 1 cm sequence then it will take a while to run!).\n\n\n## Accumulation rate\n\nNow that we have an age-depth model, we can also obtain the accumulation rates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc_rate <- summary(SlugganOut,\n  type = \"acc_rate\",\n  probs = c(0.25, 0.5, 0.75)\n)\n```\n:::\n\n\nAnd visualise them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot2::ggplot(acc_rate, aes(\n  x = age_grid, y = `50%`,\n  ymax = `75%`, ymin = `25%`)) +\n  geom_line() +\n  geom_ribbon(alpha = 0.2) +\n  theme_bw() +\n  scale_x_reverse() +\n  labs(\n    y = \"cm per year\",\n    x = \"Age (cal years BP)\"\n  )\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n# Neotoma example\n\nSo far we have used the built-in dataset from the northern hemisphere for the sake of demonstration. Let's now follow an example by pulling data from the Neotoma database and model age-depth in the southern hemisphere. Since this tutorial is focused on age-depth modelling we won't explore obtaining data from Neotoma in any detail, we will just have a look at rebuilding a chronology from Neotoma sites. The [Neotoma github repository has links](https://github.com/NeotomaDB/Current_Workshop) to recent workshops for learning how to use the `neotoma2` R package.\n\nThe following code will download site metadata from the Neotoma database and plot the sites on a map. I have also saved some example datasets in the data directory just in case the Neotoma API is under maintenance or there are download speed issues. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!file.exists(file.path(\"./data/some_sites.rds\"))) {\n  site_ids <- c(10063, 1639, 10136, 28408, 9865, 28489, 31732)\n  some_sites <- neotoma2::get_sites(site_ids)\n  saveRDS(some_sites, \"./data/some_sites.rds\")\n} else {\n  some_sites <- readRDS(\"./data/some_sites.rds\")\n  neotoma2::plotLeaflet(some_sites)\n}\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"leaflet html-widget html-fill-item\" id=\"htmlwidget-5b54b21915cda8fe2d7d\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5b54b21915cda8fe2d7d\">{\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"&copy; <a href=\\\"https://openstreetmap.org/copyright/\\\">OpenStreetMap<\\/a>,  <a href=\\\"https://opendatacommons.org/licenses/odbl/\\\">ODbL<\\/a>\"}]},{\"method\":\"addCircleMarkers\",\"args\":[[-41.33333,-45.027516,-41.359924,-54.870782,-46.89029,-30.78,-12.2167],[-71.58333,168.573484,-71.50931800000001,-67.29683799999999,37.87003,139.9,34.63338],10,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":true,\"riseOffset\":250,\"stroke\":true,\"color\":\"#03F\",\"weight\":5,\"opacity.1\":0.5,\"fill\":true,\"fillColor\":\"#03F\",\"fillOpacity\":0.2},{\"showCoverageOnHover\":true,\"zoomToBoundsOnClick\":true,\"spiderfyOnMaxZoom\":true,\"removeOutsideVisibleBounds\":true,\"spiderLegPolylineOptions\":{\"weight\":1.5,\"color\":\"#222\",\"opacity\":0.5},\"freezeAtZoom\":false},null,[\"<b>Mallin Book<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=1639>Explorer Link<\\/a>\",\"<b>Lake Kirkpatrick<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=9865>Explorer Link<\\/a>\",\"<b>Laguna Padre Laguna<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=10063>Explorer Link<\\/a>\",\"<b>Harberton<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=10136>Explorer Link<\\/a>\",\"<b>Albatross Lake<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=28408>Explorer Link<\\/a>\",\"<b>Lake Frome<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=28489>Explorer Link<\\/a>\",\"<b>Lake Malawi [project PROBE]<\\/b><br><b>Description:<\\/b> NA<br><a href=http://apps.neotomadb.org/explorer/?siteids=31732>Explorer Link<\\/a>\"],null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[-54.870782,-12.2167],\"lng\":[-71.58333,168.573484]}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nThe next chunk of code uses the metadata to filter for sites with pollen records (and some time-span, i.e., not surface samples) and downloads the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!file.exists(file.path(\"./data/some_datasets.rds\"))) {\n\n  some_datasets <- \n    neotoma2::get_datasets(some_sites, all_data = TRUE) |>\n    neotoma2::filter(datasettype == \"pollen\" & !is.na(age_range_young)) |>\n    neotoma2::get_downloads()\n  \n  saveRDS(some_datasets, \"./data/some_datasets.rds\")\n} else {\n  some_datasets <- readRDS(\"./data/some_datasets.rds\")\n}\n```\n:::\n\n\nNow we have some sites that we can explore, let's rebuild a chronology. Note, that each core may have more than one chronology associated with it. We can check how many chronologies are associated with a site by the `chronologyid`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Meta data for each chronology\ncontrols_meta <- neotoma2::chronologies(some_datasets[[1]])\n\n# Dataframe of the chronologial conrols\ncontrols <- neotoma2::chroncontrols(some_datasets[[1]]) |>\n  dplyr::arrange(depth)\n\nunique(controls$chronologyid)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]   777 33123\n```\n\n\n:::\n:::\n\nSee that this core has two chronologies? I'm picking the second one in this case, but you probably will want to see associated publications if listed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrols <- neotoma2::chroncontrols(some_datasets[[1]]) |>\n  dplyr::filter(chronologyid == 33123) |> # If there is more than on chronology\n  dplyr::arrange(depth)\n```\n:::\n\n\n\nNote that the `controls` dataframe contains all the information necessary to build a new chronology. The data are not in the identical format as the in-build Sluggan dataset, so some very minor adjustment is necessary. The Sluggan data has one field for standard deviation, the Neotoma data has an upper (`agelimitolder`) and lower (`agelimityounger`) limit. These two variables are the age (`chroncontrolage`) plus and minus the standard deviation. So the standard deviation is the difference between the age and the limits (`abs(controls$agelimityounger - controls$chroncontrolage)`). There is also no calibration curves field. Both the standard deviation and the calcurves can be added to the dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrols <- controls |>\n  dplyr::mutate(ageSds = abs(controls$agelimityounger - controls$chroncontrolage),\n                calCurves = c(\"normal\", rep(\"shcal20\", 9)))\n```\n:::\n\n\n\nIt is good practice to use `set.seed(...)` before running an age-depth model. The models are stochastic and will give slightly different results each time. For reproducibility it is good to fix the seed, and also for your own sanity when you re-run your code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1984)\n\nnewChron <- Bchron::Bchronology(\n  ages = controls$chroncontrolage,\n  ageSds = controls$ageSds,\n  calCurves = controls$calCurves,\n  positions = controls$depth,\n  positionThicknesses = controls$thickness,\n  ids = controls$chroncontrolid,\n  predictPositions=\n      seq(min(controls$depth), max(controls$depth), by=10))\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(newChron)\n```\n\n::: {.cell-output-display}\n![](neotoma-age-depth_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Your turn\n\nIn the remaining time, give a shot at building or rebuilding a chronology. If you have your own data, or would like to download a site of your choice from Neotoma, feel free to do that. [Neotoma explorer](https://apps.neotomadb.org/explorer/index.html) can help you find a site id.\n\n- bring your own data, pick one of the existing downloaded sites, or find a site on neotoma explorer.\n- Build a new chronology!\n\n\n# The end\n\n\n\n::: {.callout-tip}\n# Resources\n\nAge-depth modelling is complicated, there are many pitfalls, assumptions, and uncertainties that are often ignored. Recent developments have begun to focus on quantifying uncertainties to understand the reliability of inferences made from the data. Key papers for understanding age depth modelling include:\n\n- The Bchron [vignette](https://cran.r-project.org/web/packages/Bchron/vignettes/Bchron.html) is also a useful resource. \n\n- @blaauw2018\n\n- @parnell2008\n\n- @trachsel2017\n\n\n:::\n\n\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"neotoma-age-depth_files/libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"neotoma-age-depth_files/libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"neotoma-age-depth_files/libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"neotoma-age-depth_files/libs/leaflet-1.3.1/leaflet.css\" rel=\"stylesheet\" />\n<script src=\"neotoma-age-depth_files/libs/leaflet-1.3.1/leaflet.js\"></script>\n<link href=\"neotoma-age-depth_files/libs/leafletfix-1.0.0/leafletfix.css\" rel=\"stylesheet\" />\n<script src=\"neotoma-age-depth_files/libs/proj4-2.6.2/proj4.min.js\"></script>\n<script src=\"neotoma-age-depth_files/libs/Proj4Leaflet-1.0.1/proj4leaflet.js\"></script>\n<link href=\"neotoma-age-depth_files/libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css\" rel=\"stylesheet\" />\n<script src=\"neotoma-age-depth_files/libs/leaflet-binding-2.2.3/leaflet.js\"></script>\n<link href=\"neotoma-age-depth_files/libs/leaflet-markercluster-1.0.5/MarkerCluster.css\" rel=\"stylesheet\" />\n<link href=\"neotoma-age-depth_files/libs/leaflet-markercluster-1.0.5/MarkerCluster.Default.css\" rel=\"stylesheet\" />\n<script src=\"neotoma-age-depth_files/libs/leaflet-markercluster-1.0.5/leaflet.markercluster.js\"></script>\n<script src=\"neotoma-age-depth_files/libs/leaflet-markercluster-1.0.5/leaflet.markercluster.freezable.js\"></script>\n<script src=\"neotoma-age-depth_files/libs/leaflet-markercluster-1.0.5/leaflet.markercluster.layersupport.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}